# Downloaded Papers

1. [Measuring Massive Multitask Language Understanding](2009.03300_mmlu.pdf)  
   - Authors: Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt  
   - Year: 2020 (arXiv:2009.03300)  
   - Why relevant: Introduces MMLU, a broad multi-domain benchmark often used to test generalization and resistance to superficial fine-tuning.

2. [MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark](2406.01574_mmlu_pro.pdf)  
   - Authors: Zihan Liu, Kaiwen Zhou, et al.  
   - Year: 2024 (arXiv:2406.01574)  
   - Why relevant: Proposes a harder, contamination-reduced successor to MMLU, explicitly designed to reduce fine-tuning gains from memorization.

3. [Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them](2210.09261_bigbench_hard.pdf)  
   - Authors: Suzgun, Scales, Schärli, et al.  
   - Year: 2022 (arXiv:2210.09261)  
   - Why relevant: Defines BIG-Bench Hard (BBH), a set of adversarial tasks used to probe reasoning and robustness beyond easy fine-tuning wins.

4. [TruthfulQA: Measuring How Models Mimic Human Falsehoods](2109.07958_truthfulqa.pdf)  
   - Authors: Stephanie Lin, Jacob Hilton, Owain Evans  
   - Year: 2021 (arXiv:2109.07958)  
   - Why relevant: Benchmark for factual robustness; models often overfit to misleading patterns—useful for testing whether fine-tuning actually improves truthfulness.

5. [Think You Have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge](1803.05457_arc.pdf)  
   - Authors: Peter Clark, Isaac Cowhey, Oren Etzioni, et al.  
   - Year: 2018 (arXiv:1803.05457)  
   - Why relevant: Science exam questions requiring reasoning; historically challenging for shallow fine-tuning, useful as a finetuning-resistance probe.
